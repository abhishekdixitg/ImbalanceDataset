{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-18T16:55:12.213247Z","iopub.execute_input":"2024-01-18T16:55:12.213652Z","iopub.status.idle":"2024-01-18T16:55:24.702268Z","shell.execute_reply.started":"2024-01-18T16:55:12.21362Z","shell.execute_reply":"2024-01-18T16:55:24.700982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install lion-pytorch","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:24.704292Z","iopub.execute_input":"2024-01-18T16:55:24.704647Z","iopub.status.idle":"2024-01-18T16:55:38.222282Z","shell.execute_reply.started":"2024-01-18T16:55:24.704618Z","shell.execute_reply":"2024-01-18T16:55:38.220629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport pywt\nimport pywt.data\nimport torch\nimport numpy as np\nfrom skimage import exposure\nfrom skimage.filters import threshold_otsu\nfrom lion_pytorch import Lion  # You might need to implement or find a suitable LOA library\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Reshape\n\n\n# Step 1: Pre-processing with a combination of wavelet and Gaussian filters\ndef pre_process_image(image):\n    if len(image.shape) == 3:  # Check if the image is color\n        # Convert the image to grayscale\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray_image = image\n\n    # Apply wavelet filter (you can choose a specific wavelet)\n    coeffs = pywt.dwt2(gray_image, 'bior1.3')\n    wavelet_image = pywt.idwt2(coeffs, 'bior1.3')\n\n    # Resize the wavelet image to match the dimensions of the original image\n    wavelet_image = cv2.resize(wavelet_image, (image.shape[1], image.shape[0]))\n\n    if len(image.shape) == 3:  # Check if the image is color\n        # Apply Gaussian filter separately for each channel\n        gaussian_image = np.zeros_like(image, dtype=np.float32)\n        for i in range(image.shape[2]):\n            gaussian_image[:, :, i] = cv2.GaussianBlur(image[:, :, i].astype(np.float32), (5, 5), 0)\n    else:\n        # Apply Gaussian filter for grayscale images\n        gaussian_image = cv2.GaussianBlur(image.astype(np.float32), (5, 5), 0)\n\n    # Ensure both images have the same shape before addition\n    if len(image.shape) == 3:  # Check if the image is color\n        # Convert the grayscale image to a color image with identical channels\n        wavelet_image = cv2.cvtColor(wavelet_image.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n\n    # Combine the two filtered images (you can adjust the weights)\n    hybrid_image = 0.7 * wavelet_image.astype(np.float32) + 0.3 * gaussian_image\n\n    return hybrid_image\n\n# Step 2: Segmentation using Otsu thresholding\ndef segmentation(image):\n    # Convert to 8-bit image\n    image_8bit = cv2.convertScaleAbs(image)\n\n    # Convert to grayscale\n    gray_image = cv2.cvtColor(image_8bit, cv2.COLOR_BGR2GRAY)\n\n    # Apply Otsu thresholding\n    _, segmented_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    return segmented_image\n\n# Step 3: Feature extraction using Lion Optimization Algorithm\ndef lion_optimization_algorithm(data):\n    data_tensor = torch.tensor(data, dtype=torch.float32)\n    \n    # Create an instance of Lion optimizer\n    loa_optimizer = Lion([{'params': data_tensor}], lr=1e-4, weight_decay=1e-2)\n\n    # You may need to adjust the number of optimization steps based on your requirements\n    num_optimization_steps = 100\n\n    # Perform optimization steps\n    for _ in range(num_optimization_steps):\n        # Assuming you have a closure function for computing the loss\n        def closure():\n            # Your computation of loss goes here\n            loss = compute_loss(data_tensor)\n            return loss\n\n        loa_optimizer.step(closure)\n\n    # Get the optimized features (assuming LOA modifies the input data in-place)\n    optimized_features = data_tensor.numpy()\n\n    return optimized_features\n\ndef compute_loss(data_tensor):\n    # Replace this with your actual computation of loss based on the data_tensor\n    # For example, you might have a machine learning model, and this could be the forward pass\n    # Make sure to return a torch.Tensor representing the loss\n    loss = torch.tensor(0.0, requires_grad=True)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:38.224893Z","iopub.execute_input":"2024-01-18T16:55:38.225339Z","iopub.status.idle":"2024-01-18T16:55:42.459731Z","shell.execute_reply.started":"2024-01-18T16:55:38.225305Z","shell.execute_reply":"2024-01-18T16:55:42.458665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_files=os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/NORMAL')\n\nprint(normal_files[0:5])\nprint(normal_files[-5:])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.462204Z","iopub.execute_input":"2024-01-18T16:55:42.463014Z","iopub.status.idle":"2024-01-18T16:55:42.470465Z","shell.execute_reply.started":"2024-01-18T16:55:42.462978Z","shell.execute_reply":"2024-01-18T16:55:42.469285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Pneumonia_files=os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA')\n\nprint(Pneumonia_files[0:5])\nprint(Pneumonia_files[-5:])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.472267Z","iopub.execute_input":"2024-01-18T16:55:42.472733Z","iopub.status.idle":"2024-01-18T16:55:42.482552Z","shell.execute_reply.started":"2024-01-18T16:55:42.472693Z","shell.execute_reply":"2024-01-18T16:55:42.48134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Normal: ', len(normal_files))\nprint('Pneumonia: ',len(Pneumonia_files))","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.484317Z","iopub.execute_input":"2024-01-18T16:55:42.484731Z","iopub.status.idle":"2024-01-18T16:55:42.491172Z","shell.execute_reply.started":"2024-01-18T16:55:42.484695Z","shell.execute_reply":"2024-01-18T16:55:42.490168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Normal_label=[0]*1341\nPneumonia_label=[1]*3875","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.492513Z","iopub.execute_input":"2024-01-18T16:55:42.492845Z","iopub.status.idle":"2024-01-18T16:55:42.499997Z","shell.execute_reply.started":"2024-01-18T16:55:42.492819Z","shell.execute_reply":"2024-01-18T16:55:42.498963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('With Normal labels: ', Normal_label[0:5])\nprint('Without Pneumonia labels: ', Pneumonia_label[0:5])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.501119Z","iopub.execute_input":"2024-01-18T16:55:42.501573Z","iopub.status.idle":"2024-01-18T16:55:42.511912Z","shell.execute_reply.started":"2024-01-18T16:55:42.501543Z","shell.execute_reply":"2024-01-18T16:55:42.510848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=Normal_label + Pneumonia_label\n\nprint('Labels are: ',len(labels))\n\nprint(labels[0:5])\nprint(labels[-5:])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.513337Z","iopub.execute_input":"2024-01-18T16:55:42.514438Z","iopub.status.idle":"2024-01-18T16:55:42.523922Z","shell.execute_reply.started":"2024-01-18T16:55:42.514405Z","shell.execute_reply":"2024-01-18T16:55:42.523068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nnormal_path=('/kaggle/input/chest-xray-pneumonia/chest_xray/train/NORMAL/')\ndata=[]\n\nfor img_file in normal_files:\n    image=Image.open(normal_path +img_file)\n    image=image.resize((128,128))\n    image=image.convert('RGB')\n    image=np.array(image)\n    data.append(image)\n    \nPneumonia_path=('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/')\n\nfor img_file in Pneumonia_files:\n    image=Image.open(Pneumonia_path + img_file )\n    image=image.resize((128,128))\n    image=image.convert('RGB')\n    image=np.array(image)\n    data.append(image)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T16:55:42.528211Z","iopub.execute_input":"2024-01-18T16:55:42.52857Z","iopub.status.idle":"2024-01-18T16:57:24.472941Z","shell.execute_reply.started":"2024-01-18T16:55:42.528541Z","shell.execute_reply":"2024-01-18T16:57:24.471883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Reshape\n\ndef cnn_lstm_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 1)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Reshape((-1, 64)))\n    model.add(LSTM(100))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T17:13:16.934469Z","iopub.execute_input":"2024-01-18T17:13:16.934942Z","iopub.status.idle":"2024-01-18T17:13:16.944421Z","shell.execute_reply.started":"2024-01-18T17:13:16.934908Z","shell.execute_reply":"2024-01-18T17:13:16.942921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\n# Example Usage:\n# Load your image data\n#image_data = cv2.imread('/kaggle/input/chest-xray-pneumonia/chest_xray/train/NORMAL/IM-0115-0001.jpeg')\n\n# Step 1: Pre-processing\n#pre_processed_image = pre_process_image(data)\npre_processed_train_data = np.array([pre_process_image(img) for img in data])\n\n\n# Step 2: Segmentation\n#segmented_image = segmentation(pre_processed_image)\nsegmented_train_data = np.array([segmentation(img) for img in pre_processed_train_data])\n\n\n# Step 3: Feature extraction\n#extracted_features = lion_optimization_algorithm(segmented_image)\noptimized_train_features = lion_optimization_algorithm(segmented_train_data)\n# Assuming optimized_train_features has shape (batch_size, height, width)\noptimized_train_features = np.array([lion_optimization_algorithm(segmented_img) for segmented_img in segmented_train_data])\n\n\n# Expand dimensions to include the channel\noptimized_train_features = np.expand_dims(optimized_train_features, axis=-1)\n\n\n# Verify the shape after modification\nprint(optimized_train_features.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T17:13:20.656768Z","iopub.execute_input":"2024-01-18T17:13:20.657234Z","iopub.status.idle":"2024-01-18T17:13:45.361086Z","shell.execute_reply.started":"2024-01-18T17:13:20.657196Z","shell.execute_reply":"2024-01-18T17:13:45.359933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine the sequence length and number of features\n#sequence_length = optimized_train_features.shape[1]  # Adjust this based on your data\n#num_features = optimized_train_features.shape[2]  # Adjust this based on your data\n# Reshape the data to have the required 3D shape\n#reshaped_features = optimized_train_features.reshape(-1, sequence_length, num_features)\n\n\n# Step 4: Categorization using CNN-LSTM\ninput_shape = (128, 128, 3)  # Assuming RGB images\nnum_classes = 2  # Specify the number of classes in your dataset\n\n#model = cnn_lstm_model(input_shape, num_classes)\n\n# Convert labels to one-hot encoding\ntrain_labels_one_hot = to_categorical(labels, num_classes)\n\n# Create the model\nmodel = cnn_lstm_model(input_shape, num_classes)\n#model.add(LSTM(100, input_shape=(sequence_length, num_features)))\n# Train the model\nmodel.fit(optimized_train_features, train_labels_one_hot, epochs=10, batch_size=32, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T17:14:09.649911Z","iopub.execute_input":"2024-01-18T17:14:09.650401Z","iopub.status.idle":"2024-01-18T17:33:12.694116Z","shell.execute_reply.started":"2024-01-18T17:14:09.650362Z","shell.execute_reply":"2024-01-18T17:33:12.69292Z"},"trusted":true},"execution_count":null,"outputs":[]}]}