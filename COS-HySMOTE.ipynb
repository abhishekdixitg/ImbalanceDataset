{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T10:24:15.362494Z","iopub.execute_input":"2024-02-06T10:24:15.362980Z","iopub.status.idle":"2024-02-06T10:24:15.860563Z","shell.execute_reply.started":"2024-02-06T10:24:15.362942Z","shell.execute_reply":"2024-02-06T10:24:15.858807Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pip install smote_variants","metadata":{"execution":{"iopub.status.busy":"2024-02-06T10:24:19.261051Z","iopub.execute_input":"2024-02-06T10:24:19.261685Z","iopub.status.idle":"2024-02-06T10:24:53.667146Z","shell.execute_reply.started":"2024-02-06T10:24:19.261648Z","shell.execute_reply":"2024-02-06T10:24:53.666005Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting smote_variants\n  Downloading smote_variants-0.7.3-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from smote_variants) (1.24.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from smote_variants) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from smote_variants) (1.2.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from smote_variants) (1.3.2)\nCollecting minisom (from smote_variants)\n  Downloading MiniSom-2.3.1.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting statistics (from smote_variants)\n  Downloading statistics-1.0.3.5.tar.gz (8.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from smote_variants) (2.15.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from smote_variants) (2.15.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from smote_variants) (2.2.0)\nCollecting mkl (from smote_variants)\n  Downloading mkl-2024.0.0-py2.py3-none-manylinux1_x86_64.whl.metadata (1.4 kB)\nCollecting metric-learn (from smote_variants)\n  Downloading metric_learn-0.7.0-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from smote_variants) (0.12.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->smote_variants) (3.2.0)\nCollecting intel-openmp==2024.* (from mkl->smote_variants)\n  Downloading intel_openmp-2024.0.2-py2.py3-none-manylinux1_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: tbb==2021.* in /opt/conda/lib/python3.10/site-packages (from mkl->smote_variants) (2021.11.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->smote_variants) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->smote_variants) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->smote_variants) (2023.4)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn->smote_variants) (3.7.4)\nRequirement already satisfied: docutils>=0.3 in /opt/conda/lib/python3.10/site-packages (from statistics->smote_variants) (0.20.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->smote_variants) (2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->smote_variants) (0.42.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (3.1.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->smote_variants) (3.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->smote_variants) (3.2.2)\nDownloading smote_variants-0.7.3-py3-none-any.whl (416 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.2/416.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mkl-2024.0.0-py2.py3-none-manylinux1_x86_64.whl (200.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.7/200.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading intel_openmp-2024.0.2-py2.py3-none-manylinux1_x86_64.whl (28.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.6/28.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: minisom, statistics\n  Building wheel for minisom (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for minisom: filename=MiniSom-2.3.1-py3-none-any.whl size=10589 sha256=b2c58a2e078cd031fe4f280b2be7029249624c40400b6acf72d0851c39df53a3\n  Stored in directory: /root/.cache/pip/wheels/c7/92/d2/33bbda5f86fd8830510b16aa98c8dd420129b5cb24248fd6db\n  Building wheel for statistics (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for statistics: filename=statistics-1.0.3.5-py3-none-any.whl size=7436 sha256=1f27b0b91d7c811dae7e0beab0d5c1baca422b47a86c7abd8c71ba496f5f8b97\n  Stored in directory: /root/.cache/pip/wheels/59/bd/aa/41ad56fbb723d36e457b5be74feb992cdac2855a6999ba34a1\nSuccessfully built minisom statistics\nInstalling collected packages: minisom, intel-openmp, statistics, mkl, metric-learn, smote_variants\nSuccessfully installed intel-openmp-2024.0.2 metric-learn-0.7.0 minisom-2.3.1 mkl-2024.0.0 smote_variants-0.7.3 statistics-1.0.3.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import smote_variants as sv","metadata":{"execution":{"iopub.status.busy":"2024-02-06T10:25:28.728168Z","iopub.execute_input":"2024-02-06T10:25:28.728699Z","iopub.status.idle":"2024-02-06T10:25:30.585627Z","shell.execute_reply.started":"2024-02-06T10:25:28.728657Z","shell.execute_reply":"2024-02-06T10:25:30.584133Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"oversampler= sv.Borderline_SMOTE1()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T10:25:47.540365Z","iopub.execute_input":"2024-02-06T10:25:47.542203Z","iopub.status.idle":"2024-02-06T10:25:47.548824Z","shell.execute_reply.started":"2024-02-06T10:25:47.542144Z","shell.execute_reply":"2024-02-06T10:25:47.547797Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_files=os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/NORMAL')\n\nprint(normal_files[0:5])\nprint(normal_files[-5:])","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:41:38.678705Z","iopub.execute_input":"2024-02-03T17:41:38.679185Z","iopub.status.idle":"2024-02-03T17:41:38.850363Z","shell.execute_reply.started":"2024-02-03T17:41:38.679152Z","shell.execute_reply":"2024-02-03T17:41:38.849233Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['NORMAL2-IM-0771-0001.jpeg', 'NORMAL2-IM-1294-0001-0002.jpeg', 'IM-0675-0001.jpeg', 'NORMAL2-IM-1169-0001.jpeg', 'IM-0421-0001.jpeg']\n['NORMAL2-IM-1160-0001.jpeg', 'NORMAL2-IM-0389-0001.jpeg', 'IM-0162-0001.jpeg', 'NORMAL2-IM-1247-0001.jpeg', 'IM-0219-0001.jpeg']\n","output_type":"stream"}]},{"cell_type":"code","source":"Pneumonia_files=os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA')\n\nprint(Pneumonia_files[0:5])\nprint(Pneumonia_files[-5:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Normal_label=[0]*1341\nPneumonia_label=[1]*3875","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('With Normal labels: ', Normal_label[0:5])\nprint('Without Pneumonia labels: ', Pneumonia_label[0:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=Normal_label + Pneumonia_label\n\nprint('Labels are: ',len(labels))\n\nprint(labels[0:5])\nprint(labels[-5:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nnormal_path=('/kaggle/input/chest-xray-pneumonia/chest_xray/train/NORMAL/')\ndata=[]\n\nfor img_file in normal_files:\n    image=Image.open(normal_path +img_file)\n    image=image.resize((128,128))\n    image=image.convert('RGB')\n    image=np.array(image)\n    data.append(image)\n    \nPneumonia_path=('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/')\n\nfor img_file in Pneumonia_files:\n    image=Image.open(Pneumonia_path + img_file )\n    image=image.resize((128,128))\n    image=image.convert('RGB')\n    image=np.array(image)\n    data.append(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ..base import OverSampling, NearestNeighborsWithMetricTensor, coalesce\nfrom .._logger import logger\nfrom ._smote import SMOTE\nimport math\n_logger = logger\n\n__all__ = [\"SMOTE_Cosine_Eng\"]\n\n\nclass SMOTE_Cosine_Eng(OverSampling):\n    categories = [\n        OverSampling.cat_noise_removal,  # applies noise removal\n        OverSampling.cat_uses_classifier,  # uses some advanced classifier\n        OverSampling.cat_sample_ordinary,  # sampling is done in the SMOTE scheme\n        OverSampling.cat_extensive,  # adds minority samples only\n        OverSampling.cat_metric_learning,\n    ]  # metric learning is applicable (uses nearest neighbors)\n    sample_good = 1\n    sample_lonely = 0\n    sample_bad = -1\n    def __init__(\n        self, proportion=1.0,\n            n_neighbors=5, *,\n            n_jobs=1,\n            random_state=None,\n            nn_params=None,\n            n_iters=100,\n            max_depth=30,\n            **_kwargs\n    ):\n        \n        super().__init__(random_state=random_state)\n        self.check_greater_or_equal(proportion, \"proportion\", 0)\n        self.check_greater_or_equal(n_neighbors, \"n_neighbors\", 1)\n        self.check_n_jobs(n_jobs, \"n_jobs\")\n\n        self.proportion = proportion\n        self.n_neighbors = n_neighbors\n        self.max_depth = max_depth\n        self.nn_params = coalesce(nn_params, {})\n        self.n_iters = n_iters\n        self.n_jobs = n_jobs\n\n    @classmethod\n    def parameter_combinations(cls, raw=False):\n        \"\"\"\n        Generates reasonable parameter combinations.\n\n        Returns:\n            list(dict): a list of meaningful parameter combinations\n        \"\"\"\n        parameter_combinations = {\n            \"proportion\": [0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0],\n            \"n_neighbors\": [3, 5, 7],\n        }\n        return cls.generate_parameter_combinations(parameter_combinations, raw)\n\n    def epsilon_neighborhood_graph(self, X, epsilon=0.5):\n        \"\"\"\n        Build an epsilon-neighborhood graph based on cosine similarity.\n\n        Args:\n            X (np.ndarray): Data points.\n            epsilon (float): Threshold for considering neighbors.\n\n        Returns:\n            np.ndarray: Binary matrix representing the epsilon-neighborhood graph.\n        \"\"\"\n        n_samples = X.shape[0]\n        graph = np.zeros((n_samples, n_samples), dtype=bool)\n\n        for i in range(n_samples):\n            for j in range(i + 1, n_samples):\n                cosine_similarity = np.dot(X[i], X[j]) / (np.linalg.norm(X[i]) * np.linalg.norm(X[j]))\n                if cosine_similarity >= epsilon:\n                    graph[i, j] = graph[j, i] = True\n\n        return graph\n    def first_majority_index(self, nearest_ys, no_majority_result=-1):\n        \"\"\"\n        Finds the index of the first majority element in the nearest_ys per row.\n\n        Args:\n            nearest_ys (np.ndarray): label of the nearest neighbours\n            no_majority_result (int): result if there is no majority item in a row\n        Returns:\n            np.array: the i-th element of the array is the index of the first majority label\n                        in the i-th row of the nearest_ys, or no_majority_result if the row\n                        contains no majority labels.\n        \"\"\"\n        mask = nearest_ys == self.maj_label\n        return np.where(mask.any(axis=1), mask.argmax(axis=1), no_majority_result)\n    def sampling_formula(self, u, v, x):  # pylint: disable=invalid-name\n        \"\"\"\n        The sampling formula.\n\n        Args:\n            u (np.array): the u vector\n            v (np.array): the v vector\n            x (np.array): the x vector\n\n        Returns:\n            np.array: the generated samples\n        \"\"\"\n        uu = u + self.random_state.random_sample(u.shape) * 0.3 * (\n            u - v\n        )  # pylint: disable=invalid-name\n\n        return x + self.random_state.random_sample(x.shape) * 0.5 * (uu - x)\n\n    def generate_samples(\n        self, *, X, y, X_maj, X_min, n_to_sample, nn_maj_ind, composite_ind\n    ):\n        \"\"\"\n        Generate samples\n\n        Args:\n            X (np.array): all training vectors\n            y (np.array): the target labels\n            X_maj (np.array): majority vectors\n            X_min (np.array): minority vectors\n            n_to_sample (int): number of samples to generate\n            nn_maj_ind (np.array): majority neighborhood structure\n            composite_ind (np.array): composite neighborhood structure\n\n        Returns:\n            np.array: the generated samples\n        \"\"\"\n        minority_indices = np.where(y == self.min_label)[0]\n\n        base_ind = self.random_state.choice(\n            np.arange(len(minority_indices)), n_to_sample\n        )\n\n        neigh_ind = self.random_state.choice(\n            np.arange(0, nn_maj_ind.shape[1]), n_to_sample\n        )\n\n        min_neigh_ind = self.random_state.choice(\n            np.arange(1, composite_ind.shape[1]), n_to_sample\n        )\n\n        return self.sampling_formula(\n            u=X[minority_indices[base_ind]],\n            v=X_maj[nn_maj_ind[base_ind, neigh_ind]],\n            x=X_min[composite_ind[base_ind, min_neigh_ind]],\n        )\n    @staticmethod\n    def noise_threshold(w, th):\n        \"\"\"\n        Creating the noise mask\n\n        Args:\n            w (np.array): the weights\n            th (float): the threshold\n\n        Returns:\n            np.array: the mask\n        \"\"\"\n        noise = np.repeat(False, len(w))\n        noise[w > th] = True\n        return noise\n\n    def noise_detection(self, X, y):\n        \"\"\"\n        Implementation of Algorithm 3.\n\n        Args:\n            X (np.ndarray): training set\n            y (np.array): target labels\n\n        Returns:\n            (np.array, np.array): Mask of noise for the minority and the majority\n            samples, respectively.\n        \"\"\"\n\n        w = self.boosted_weights(X, y)\n\n        w_min = w[y == self.min_label]\n        w_maj = w[y == self.maj_label]\n\n        n_samples = len(y)\n\n        # noise thresholds for majority and minority classes\n        th_min = (2 * len(w_maj)) / (n_samples**2)\n        th_maj = (2 * len(w_min)) / (n_samples**2)\n\n        noise_mask_min = SMOTE_Cosine_Eng.noise_threshold(w_min, th_min)\n        noise_mask_maj = SMOTE_Cosine_Eng.noise_threshold(w_maj, th_maj)\n\n        return noise_mask_min, noise_mask_maj\n    def sampling_algorithm(\n            self, X, y\n    ):  # pylint: disable=too-many-locals,too-many-statements\n        \"\"\"\n        Does the sample generation according to the class parameters.\n\n        TODO: break into smaller functions\n\n        Args:\n            X (np.ndarray): training set\n            y (np.array): target labels\n        Returns:\n            (np.ndarray, np.array): the extended training set and target labels\n        \"\"\"\n\n        # determine the number of samples to generate\n        n_to_sample = self.det_n_to_sample(self.proportion)\n\n        if n_to_sample == 0:\n            return self.return_copies(X, y, \"Sampling is not needed.\")\n\n        # use logging in the below format\n        _logger.info(\"%s: sampling\", self.__class__.__name__)\n\n        # Step 1: MinMaxScaler - Rescaling is ignored at the suggestion of the\n        # author of the algorithm.\n\n        # Step2: separating the classes\n        X_min = X[y == self.min_label]\n        X_maj = X[y == self.maj_label]\n\n        # Step 3-4: creating masks of noise samples\n        noise_mask_min, noise_mask_maj = self.noise_detection(X, y)\n\n        # Start: Algorithm 4\n        n_min = len(X_min)\n        n_maj = len(X_maj)\n\n        # paper: floor, R implementation: ceil\n        k_max = math.floor(n_maj / n_min)\n\n        # non-noise samples and labels\n        X_min_not_noise = X_min[~noise_mask_min]  # pylint: disable=invalid-name\n        X_maj_not_noise = X_maj[~noise_mask_maj]  # pylint: disable=invalid-name\n        X_not_noise = np.concatenate(  # pylint: disable=invalid-name\n            [X_min_not_noise, X_maj_not_noise]\n        )\n        y_not_noise = np.concatenate(\n            [\n                np.repeat(self.min_label, X_min_not_noise.shape[0]),\n                np.repeat(self.maj_label, X_maj_not_noise.shape[0]),\n            ]\n        )\n\n        # fitting the model\n        # paper: X^good_pos makes no sense, R code: x_notnoise\n        nn_params = {**self.nn_params}\n        nn_params[\"metric_tensor\"] = self.metric_tensor_from_nn_params(nn_params, X, y)\n\n        # all data points are noise\n        if len(X_not_noise) == 0:\n            smote = SMOTE(proportion=self.proportion, nn_params=self.nn_params)\n            return smote.sample(X, y)\n\n        n_neighbors = min([len(X_not_noise), k_max + 1])\n        nnmt = NearestNeighborsWithMetricTensor(\n            n_neighbors=n_neighbors, n_jobs=self.n_jobs, **nn_params\n        )\n\n        nnmt.fit(X_not_noise)\n        indices = nnmt.kneighbors(X_min, n_neighbors=n_neighbors, return_distance=False)\n\n        # Build epsilon-neighborhood graph based on cosine similarity\n        epsilon_graph = self.epsilon_neighborhood_graph(X_min, epsilon=0.5)\n        composite_ind = epsilon_graph.argsort(axis=1)\n        # number of the positive neighbors until the first negative one (per row)\n        k_arr = self.first_majority_index(\n            y_not_noise[indices], no_majority_result=k_max + 1\n        )\n\n        # decrease one if the sample is not a noise\n        k_arr[~noise_mask_min] -= 1\n        # in case there is a majority sample at 0 distance\n        k_arr[k_arr == -1] = 0\n        # all neighbours are positive\n        k_max = min(indices.shape[1], k_max)\n        k_arr[k_arr > k_max] = k_max\n\n        # setting the labels of the samples of X_pos\n        fl_arr = np.empty(n_min, int)\n        fl_arr[k_arr > 0] = SMOTE_Cosine_Eng.sample_good\n        fl_arr[(k_arr == 0) & noise_mask_min] = SMOTE_Cosine_Eng.sample_bad\n        fl_arr[(k_arr == 0) & (~noise_mask_min)] = SMOTE_Cosine_Eng.sample_lonely\n        # End: Alg 4\n\n        # Step 5: n_to_sample - done\n        # Step 6: computing n_to_sample per samples\n        n_min = len(X_min)\n\n        # number of good and lonely samples\n        n_good_min = np.sum(fl_arr == SMOTE_Cosine_Eng.sample_good)\n        n_lonely_min = np.sum(fl_arr == SMOTE_Cosine_Eng.sample_lonely)\n\n        # prevent errors (missing from the original) no good or lonely\n        # samples => give chance to the others\n        if n_good_min + n_lonely_min == 0:\n            samples = self.generate_samples(\n                X=X,\n                y=y,\n                X_maj=X_maj,\n                X_min=X_min,\n                n_to_sample=n_to_sample,\n                nn_maj_ind=nnmt,\n                composite_ind=composite_ind,\n            )\n            return samples\n            #smote = SMOTE(proportion=self.proportion, nn_params=self.nn_params)\n            #return smote.sample(X, y)\n\n        n_to_sample_per_sample = math.ceil(n_to_sample / (n_good_min + n_lonely_min))\n        C = np.zeros(n_min, int)  # pylint: disable=invalid-name\n        C[fl_arr == SMOTE_Cosine_Eng.sample_good] = n_to_sample_per_sample\n        C[fl_arr == SMOTE_Cosine_Eng.sample_lonely] = n_to_sample_per_sample\n\n        # Step 8: correcting the number of samples to be generated to achieve the desired balance\n        # paper: ceil => n_to_sample-p.sum(C) zero or negative\n        # we probably have too many samples because of the ceil\n        diff = np.sum(C) - n_to_sample\n        if diff > 0:\n            good_and_lonely_ind = np.where(\n                (fl_arr == SMOTE_Cosine_Eng.sample_good) | (fl_arr == SMOTE_Cosine_Eng.sample_lonely)\n            )[0]\n            selected_ind = self.random_state.choice(\n                good_and_lonely_ind, diff, replace=len(good_and_lonely_ind) < diff\n            )\n            C[selected_ind] -= 1\n\n        # Step 9: sample generation\n        synt_sample_list = []\n        for i in range(0, n_min):\n            if fl_arr[i] == SMOTE_Cosine_Eng.sample_lonely:\n                for j in range(C[i]):\n                    synt_sample_list.append(X_min[i].copy())\n            elif fl_arr[i] == SMOTE_Cosine_Eng.sample_good and C[i] > 0:\n                nn = (\n                    indices[i, 0 : k_arr[i]]\n                    if noise_mask_min[i]\n                    else indices[i, 1 : k_arr[i] + 1]\n                )\n\n                if len(nn) > 0:\n                    k_ids = self.random_state.choice(nn, C[i])\n                    for j in k_ids:\n                        synt_sample_list.append(\n                            self.sample_between_points(X_min[i], X_not_noise[j])\n                        )\n\n        # Step 11-12: merging the original samples and the synthetic ones\n        X_synt_samples = np.array(synt_sample_list)  # pylint: disable=invalid-name\n        X_samples = np.vstack([X, X_synt_samples])  # pylint: disable=invalid-name\n\n        return (\n            X_samples,\n            np.hstack([y, np.hstack([self.min_label] * len(synt_sample_list))]),\n        )\n    def boosted_weights(self, X, y):\n        \"\"\"\n        Implementation of Algorithm 2.\n\n        Args:\n            X (np.ndarray): training set\n            y (np.array): target labels\n\n        Returns:\n            (np.array): weight of the samples\n        \"\"\"\n        n_samples = X.shape[0]\n        w = np.repeat(1.0 / n_samples, n_samples)\n\n        # based on boosted_weights.R\n        clf = DecisionTreeClassifier(\n            random_state=self._random_state_init,\n            max_depth=self.max_depth,\n            min_samples_split=3,\n        )\n\n        for _ in range(0, self.n_iters):\n            clf.fit(X, y, sample_weight=w)\n            predicted_labels = clf.predict(X)\n\n            w_error = sum(w[predicted_labels != y])\n\n            # no zero check in the original code (samples are not_noise)\n            if w_error == 0:\n                return np.zeros(n_samples, float)\n\n            alpha = 0.5 * np.log((1 - w_error) / w_error)\n            w[predicted_labels != y] = w[predicted_labels != y] * np.exp(alpha)\n\n            w = w / sum(w)\n\n        return w\n    def get_params(self, deep=False):\n        \"\"\"\n        Returns:\n            dict: the parameters of the current sampling object\n        \"\"\"\n        return {\n            \"proportion\": self.proportion,\n            \"n_neighbors\": self.n_neighbors,\n            \"n_jobs\": self.n_jobs,\n            **OverSampling.get_params(self),\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom skimage import io\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom your_module import SMOTE_Cosine_Eng  # Import your SMOTE-Cosine module\n\n# Load image data (as an example)\ndata = fetch_openml(name='mnist_784', version=1)\nX, y = data['data'], data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Reshape data to images (this is a simplified example, actual reshaping may vary)\nX_train_images = X_train.reshape(-1, 28, 28)\nX_test_images = X_test.reshape(-1, 28, 28)\n\n# Create an instance of SMOTE_Cosine_Eng\nsmote_cosine = SMOTE_Cosine_Eng()\n\n# Apply SMOTE-Cosine oversampling to image data\nX_train_oversampled, y_train_oversampled = smote_cosine.fit_resample(X_train_images, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}